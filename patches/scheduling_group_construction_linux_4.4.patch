diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 90a7f66..691ab8f 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -6385,61 +6385,66 @@ build_overlap_sched_groups(struct sched_domain *sd, int cpu)
 	struct sd_data *sdd = sd->private;
 	struct sched_domain *sibling;
 	int i;
+	int tries;
 
 	cpumask_clear(covered);
 
-	for_each_cpu(i, span) {
-		struct cpumask *sg_span;
-
-		if (cpumask_test_cpu(i, covered))
-			continue;
+	for(tries = 0; tries < 2; tries++) {
+		for_each_cpu(i, span) {
+			struct cpumask *sg_span;
+			if(tries == 0 && i != cpu)
+			    continue;
 
-		sibling = *per_cpu_ptr(sdd->sd, i);
+			if (cpumask_test_cpu(i, covered))
+			    continue;
 
-		/* See the comment near build_group_mask(). */
-		if (!cpumask_test_cpu(i, sched_domain_span(sibling)))
-			continue;
+			sibling = *per_cpu_ptr(sdd->sd, i);
 
-		sg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),
-				GFP_KERNEL, cpu_to_node(cpu));
+			/* See the comment near build_group_mask(). */
+			if (!cpumask_test_cpu(i, sched_domain_span(sibling)))
+			   continue;
 
-		if (!sg)
-			goto fail;
+			sg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),
+			    GFP_KERNEL, cpu_to_node(cpu));
 
-		sg_span = sched_group_cpus(sg);
-		if (sibling->child)
-			cpumask_copy(sg_span, sched_domain_span(sibling->child));
-		else
-			cpumask_set_cpu(i, sg_span);
+			if (!sg)
+			    goto fail;
 
-		cpumask_or(covered, covered, sg_span);
+			sg_span = sched_group_cpus(sg);
+			if (sibling->child)
+			    cpumask_copy(sg_span, sched_domain_span(sibling->child));
+			else
+				cpumask_set_cpu(i, sg_span);
 
-		sg->sgc = *per_cpu_ptr(sdd->sgc, i);
-		if (atomic_inc_return(&sg->sgc->ref) == 1)
-			build_group_mask(sd, sg);
+			cpumask_or(covered, covered, sg_span);
 
-		/*
-		 * Initialize sgc->capacity such that even if we mess up the
-		 * domains and no possible iteration will get us here, we won't
-		 * die on a /0 trap.
-		 */
-		sg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sg_span);
+			sg->sgc = *per_cpu_ptr(sdd->sgc, i);
+			if (atomic_inc_return(&sg->sgc->ref) == 1)
+			    build_group_mask(sd, sg);
 
-		/*
-		 * Make sure the first group of this domain contains the
-		 * canonical balance cpu. Otherwise the sched_domain iteration
-		 * breaks. See update_sg_lb_stats().
-		 */
-		if ((!groups && cpumask_test_cpu(cpu, sg_span)) ||
-		    group_balance_cpu(sg) == cpu)
-			groups = sg;
+			/*
+			 * Initialize sgc->capacity such that even if we mess up the
+			 * domains and no possible iteration will get us here, we won't
+			 * die on a /0 trap.
+			 */
+			sg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sg_span);
 
-		if (!first)
-			first = sg;
-		if (last)
-			last->next = sg;
-		last = sg;
-		last->next = first;
+			/*
+			 * Make sure the first group of this domain contains the
+			 * canonical balance cpu. Otherwise the sched_domain iteration
+			 * breaks. See update_sg_lb_stats().
+			 */
+			if ((!groups && cpumask_test_cpu(cpu, sg_span)) ||
+				group_balance_cpu(sg) == cpu)
+		            groups = sg;
+
+			if (!first)
+			    first = sg;
+			if (last)
+			    last->next = sg;
+			last = sg;
+			last->next = first;
+		}
 	}
 	sd->groups = groups;
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 09810e2..fa59e4e 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6872,36 +6872,7 @@ static int active_load_balance_cpu_stop(void *data);
 
 static int should_we_balance(struct lb_env *env)
 {
-	struct sched_group *sg = env->sd->groups;
-	struct cpumask *sg_cpus, *sg_mask;
-	int cpu, balance_cpu = -1;
-
-	/*
-	 * In the newly idle case, we will allow all the cpu's
-	 * to do the newly idle load balance.
-	 */
-	if (env->idle == CPU_NEWLY_IDLE)
-		return 1;
-
-	sg_cpus = sched_group_cpus(sg);
-	sg_mask = sched_group_mask(sg);
-	/* Try to find first idle cpu */
-	for_each_cpu_and(cpu, sg_cpus, env->cpus) {
-		if (!cpumask_test_cpu(cpu, sg_mask) || !idle_cpu(cpu))
-			continue;
-
-		balance_cpu = cpu;
-		break;
-	}
-
-	if (balance_cpu == -1)
-		balance_cpu = group_balance_cpu(sg);
-
-	/*
-	 * First idle cpu or the first cpu(busiest) in this sched group
-	 * is eligible for doing load balancing at this and above domains.
-	 */
-	return balance_cpu == env->dst_cpu;
+	return 1;
 }
 
 /*
